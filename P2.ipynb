{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA available: True\n",
      "CUDA version: 12.1\n",
      "Current device: 0\n",
      "Device name: NVIDIA GeForce GTX 1660 Ti\n"
     ]
    }
   ],
   "source": [
    "import gymnasium as gym\n",
    "import math\n",
    "import random\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import namedtuple, deque\n",
    "from itertools import count\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "env = gym.make(\"CartPole-v1\")\n",
    "\n",
    "# set up matplotlib\n",
    "is_ipython = 'inline' in matplotlib.get_backend()\n",
    "if is_ipython:\n",
    "    from IPython import display\n",
    "\n",
    "plt.ion()\n",
    "\n",
    "# if GPU is to be used\n",
    "device = torch.device(\n",
    "    \"cuda\" if torch.cuda.is_available() else\n",
    "    \"mps\" if torch.backends.mps.is_available() else\n",
    "    \"cpu\"\n",
    ")\n",
    "\n",
    "print(\"CUDA available:\", torch.cuda.is_available())\n",
    "print(\"CUDA version:\", torch.version.cuda)\n",
    "print(\"Current device:\", torch.cuda.current_device())\n",
    "print(\"Device name:\", torch.cuda.get_device_name(0) if torch.cuda.is_available() else \"No GPU available\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Transition = namedtuple('Transition',\n",
    "                        ('state', 'action', 'next_state', 'reward'))\n",
    "\n",
    "\n",
    "class ReplayMemory(object):\n",
    "\n",
    "    def __init__(self, capacity):\n",
    "        self.memory = deque([], maxlen=capacity)\n",
    "\n",
    "    def push(self, *args):\n",
    "        \"\"\"Save a transition\"\"\"\n",
    "        self.memory.append(Transition(*args))\n",
    "\n",
    "    def sample(self, batch_size):\n",
    "        return random.sample(self.memory, batch_size)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.memory)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Replay memory function is essential to optimize the agent by providing past transition data to ensure efficiency in the agent's learning. It uses a circular buffer to store transition objects which hold the states that the agent sees as it does its actions. Once there are enough transitions to fill a batch, the optimizer randomly samples the transitions to train the agent. This reduces the potential of the agent diverging due to correlation between transitions that are sampled sequentially."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DQN(nn.Module):\n",
    "    \n",
    "    def __init__(self, n_observations, n_actions):\n",
    "        super(DQN, self).__init__()\n",
    "        self.layer1 = nn.Linear(n_observations, 128)\n",
    "        self.layer2 = nn.Linear(128, 128)\n",
    "        self.layer3 = nn.Linear(128, n_actions)\n",
    "\n",
    "    # Called with either one element to determine next action, or a batch\n",
    "    # during optimization. Returns tensor([[left0exp,right0exp]...]).\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.layer1(x))\n",
    "        x = F.relu(self.layer2(x))\n",
    "        return self.layer3(x)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The DQN function creates the structure of the DQN. It takes the agent’s current state, passes it through a layer which maps it to 128 and eventually maps it to the size of the actions that the agent can take which in this case should be two, left and right. Between each layer it uses a relu in order to prevent the layers from decomposing into a single layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# BATCH_SIZE is the number of transitions sampled from the replay buffer\n",
    "# GAMMA is the discount factor as mentioned in the previous section\n",
    "# EPS_START is the starting value of epsilon\n",
    "# EPS_END is the final value of epsilon\n",
    "# EPS_DECAY controls the rate of exponential decay of epsilon, higher means a slower decay\n",
    "# TAU is the update rate of the target network\n",
    "# LR is the learning rate of the ``AdamW`` optimizer\n",
    "BATCH_SIZE = 128\n",
    "GAMMA = 0.99\n",
    "EPS_START = 0.9\n",
    "EPS_END = 0.05\n",
    "EPS_DECAY = 1000\n",
    "TAU = 0.005\n",
    "LR = 1e-4\n",
    "\n",
    "# Get number of actions from gym action space\n",
    "n_actions = env.action_space.n\n",
    "# Get the number of state observations\n",
    "state, info = env.reset()\n",
    "n_observations = len(state)\n",
    "\n",
    "policy_net = DQN(n_observations, n_actions).to(device)\n",
    "target_net = DQN(n_observations, n_actions).to(device)\n",
    "target_net.load_state_dict(policy_net.state_dict())\n",
    "\n",
    "optimizer = optim.AdamW(policy_net.parameters(), lr=LR, amsgrad=True)\n",
    "memory = ReplayMemory(10000)\n",
    "\n",
    "\n",
    "steps_done = 0\n",
    "\n",
    "\n",
    "def select_action(state):\n",
    "    global steps_done\n",
    "    sample = random.random()\n",
    "    eps_threshold = EPS_END + (EPS_START - EPS_END) * \\\n",
    "        math.exp(-1. * steps_done / EPS_DECAY)\n",
    "    steps_done += 1\n",
    "    if sample > eps_threshold:\n",
    "        with torch.no_grad():\n",
    "            # t.max(1) will return the largest column value of each row.\n",
    "            # second column on max result is index of where max element was\n",
    "            # found, so we pick action with the larger expected reward.\n",
    "            return policy_net(state).max(1).indices.view(1, 1)\n",
    "    else:\n",
    "        return torch.tensor([[env.action_space.sample()]], device=device, dtype=torch.long)\n",
    "\n",
    "\n",
    "episode_durations = []\n",
    "\n",
    "\n",
    "def plot_durations(show_result=False):\n",
    "    plt.figure(1)\n",
    "    durations_t = torch.tensor(episode_durations, dtype=torch.float)\n",
    "    if show_result:\n",
    "        plt.title('Result')\n",
    "    else:\n",
    "        plt.clf()\n",
    "        plt.title('Training...')\n",
    "    plt.xlabel('Episode')\n",
    "    plt.ylabel('Duration')\n",
    "    plt.plot(durations_t.numpy())\n",
    "    # Take 100 episode averages and plot them too\n",
    "    if len(durations_t) >= 100:\n",
    "        means = durations_t.unfold(0, 100, 1).mean(1).view(-1)\n",
    "        means = torch.cat((torch.zeros(99), means))\n",
    "        plt.plot(means.numpy())\n",
    "\n",
    "    plt.pause(0.001)  # pause a bit so that plots are updated\n",
    "    if is_ipython:\n",
    "        if not show_result:\n",
    "            display.display(plt.gcf())\n",
    "            display.clear_output(wait=True)\n",
    "        else:\n",
    "            display.display(plt.gcf())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This next part of the code is where they define some of their parameters of the network like their learning rate and their reward decay and some initialization like setting the replay memory capacity. It is also here that they create the policy and target network which is used to help stabilize the policy network’s learning using the q values from the target network whose q values stay stable for longer.\n",
    "\n",
    "The select action function is where they define their epsilon greedy algorithm. It determines the next action that the agent takes which could either be from the policy that they have learned or a random action. The probablity that the agent takes a random action is decaying by their defined rate of EPS_DECAY (1000). This allows the agent explore its environment when it hasn't learned much but over time when it learns more it will take the action with the highest rewards determined by their policy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def optimize_model():\n",
    "    if len(memory) < BATCH_SIZE:\n",
    "        return\n",
    "    transitions = memory.sample(BATCH_SIZE)\n",
    "    # Transpose the batch (see https://stackoverflow.com/a/19343/3343043 for\n",
    "    # detailed explanation). This converts batch-array of Transitions\n",
    "    # to Transition of batch-arrays.\n",
    "    batch = Transition(*zip(*transitions))\n",
    "\n",
    "    # Compute a mask of non-final states and concatenate the batch elements\n",
    "    # (a final state would've been the one after which simulation ended)\n",
    "    non_final_mask = torch.tensor(tuple(map(lambda s: s is not None,\n",
    "                                          batch.next_state)), device=device, dtype=torch.bool)\n",
    "    non_final_next_states = torch.cat([s for s in batch.next_state\n",
    "                                                if s is not None])\n",
    "    state_batch = torch.cat(batch.state)\n",
    "    action_batch = torch.cat(batch.action)\n",
    "    reward_batch = torch.cat(batch.reward)\n",
    "\n",
    "    # Compute Q(s_t, a) - the model computes Q(s_t), then we select the\n",
    "    # columns of actions taken. These are the actions which would've been taken\n",
    "    # for each batch state according to policy_net\n",
    "    state_action_values = policy_net(state_batch).gather(1, action_batch)\n",
    "\n",
    "    # Compute V(s_{t+1}) for all next states.\n",
    "    # Expected values of actions for non_final_next_states are computed based\n",
    "    # on the \"older\" target_net; selecting their best reward with max(1).values\n",
    "    # This is merged based on the mask, such that we'll have either the expected\n",
    "    # state value or 0 in case the state was final.\n",
    "    next_state_values = torch.zeros(BATCH_SIZE, device=device)\n",
    "    with torch.no_grad():\n",
    "        next_state_values[non_final_mask] = target_net(non_final_next_states).max(1).values\n",
    "    # Compute the expected Q values\n",
    "    expected_state_action_values = (next_state_values * GAMMA) + reward_batch\n",
    "\n",
    "    # Compute Huber loss\n",
    "    criterion = nn.SmoothL1Loss()\n",
    "    loss = criterion(state_action_values, expected_state_action_values.unsqueeze(1))\n",
    "\n",
    "    # Optimize the model\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    # In-place gradient clipping\n",
    "    torch.nn.utils.clip_grad_value_(policy_net.parameters(), 100)\n",
    "    optimizer.step()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This portion of the code is where they do their optimization which runs once there are enough transitions in the replay memory to fill up a batch.\n",
    "\n",
    "\n",
    "It takes the batches which are randomly sampled and seperate them into state, action, and reward batches. They then compute the Q values and action values for the current state and action batch and store it in state_action_values.\n",
    "\n",
    "It then finds the expected Q values using the target network and the bellman equation which they explained when going over their DQN algorithm. Then the Huber loss is calculated which is used during back propagation to mimimize their errors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if torch.cuda.is_available() or torch.backends.mps.is_available():\n",
    "    num_episodes = 600\n",
    "else:\n",
    "    num_episodes = 50\n",
    "\n",
    "for i_episode in range(num_episodes):\n",
    "    # Initialize the environment and get its state\n",
    "    state, info = env.reset()\n",
    "    state = torch.tensor(state, dtype=torch.float32, device=device).unsqueeze(0)\n",
    "    for t in count():\n",
    "        action = select_action(state)\n",
    "        observation, reward, terminated, truncated, _ = env.step(action.item())\n",
    "        reward = torch.tensor([reward], device=device)\n",
    "        done = terminated or truncated\n",
    "\n",
    "        if terminated:\n",
    "            next_state = None\n",
    "        else:\n",
    "            next_state = torch.tensor(observation, dtype=torch.float32, device=device).unsqueeze(0)\n",
    "\n",
    "        # Store the transition in memory\n",
    "        memory.push(state, action, next_state, reward)\n",
    "\n",
    "        # Move to the next state\n",
    "        state = next_state\n",
    "\n",
    "        # Perform one step of the optimization (on the policy network)\n",
    "        optimize_model()\n",
    "\n",
    "        # Soft update of the target network's weights\n",
    "        # θ′ ← τ θ + (1 −τ )θ′\n",
    "        target_net_state_dict = target_net.state_dict()\n",
    "        policy_net_state_dict = policy_net.state_dict()\n",
    "        for key in policy_net_state_dict:\n",
    "            target_net_state_dict[key] = policy_net_state_dict[key]*TAU + target_net_state_dict[key]*(1-TAU)\n",
    "        target_net.load_state_dict(target_net_state_dict)\n",
    "\n",
    "        if done:\n",
    "            episode_durations.append(t + 1)\n",
    "            plot_durations()\n",
    "            break\n",
    "\n",
    "print('Complete')\n",
    "plot_durations(show_result=True)\n",
    "plt.ioff()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is where they show how the training episodes have been implementated for the network. For each new episode they find the initial state tensor and use it to determine the action that the agent would respond with using the select action function that was define before. Once the action is completed the observation is used to find the state of the agent as a result of their action and the current transition is stored in the replay memory if the action was not terminated. It then moves to the next state and optimizes the policy model. Then the target network is updated based on TAU which dictates how often it is updated allowing the Q values that the expected action values are computed from can remain semi-stable to increase stability in training, and finally the episode concludes and the next can start."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PROBLEM 2\n",
    "\n",
    "DQN, short for Deep Q-Network, is a reinforcement learning algorithm where an agent learns to make optimal decisions based on the environment it is interacting with. An agent in this context refers to the “decision maker” that must interact with the given environment in order to achieve a goal such as keeping the car that has the goal of keeping the pole upright. The environment represents the external system where the agent interacts and provides the state (a set of variables describing the system at a given time). \n",
    "\n",
    "The agent then chooses an action based on the current state, with the hopes of influencing the environment toward achieving its goal. In the cart-pole problem, the actions are very simple, either moving the cart to the left or right. Each action modifies the environment and leads to the next state. Along with the next state, the agent receives a reward, which is a scalar value that provides feedback on the quality of the action. In the pole car problem for example, the agent earns a reward for every time step the pole remains upright. If the pole falls, the episode ends with a negative or zero reward, meaning failure. \n",
    "\n",
    "Over time, the DQN learns to associate specific states with corresponding actions that can maximize the reward in the long term. In the cart-pole problem, this results in the agent learning to balance the pole by correctly moving the cart in response to the position of the and rotation of the pole.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#PROBLEM 3\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Problem 4 : \n",
    "\n",
    "Frozen Lake (Toy Text) - https://gymnasium.farama.org/environments/toy_text/frozen_lake/\n",
    "\n",
    "    State:\n",
    "\n",
    "        Each state is where the agent is in the 4x4 map which is described as currentrow * numberofColumns + currentCollumn\n",
    "\n",
    "    Action:\n",
    "\n",
    "        There are 4 valid actions to move left ,right,up, or down\n",
    "\n",
    "    Environment: \n",
    "\n",
    "        the environment is a 4x4 grid. There are a total of 3 types of squares: a regular grid, a frozen lake, and a goal square with the reward changing depending on which tile the agent is interacting with.\n",
    "\n",
    "    Reward:\n",
    "\n",
    "        The agent gains a reward when it reaches the goal and no rewards for a regular snow grid or a frozen lake. If the agent steps on a frozen lake then the episode ends same with reaching the goal.\n",
    "\n",
    "Car Racing (BOX2D) - https://gymnasium.farama.org/environments/box2d/car_racing/#\n",
    "\n",
    "    State:\n",
    "\n",
    "        The state would the current position the car is in on the 96x96 image. It would have the car's velocity, grip level for each of its tires, and its current steering angle\n",
    "\n",
    "    Action:\n",
    "\n",
    "        There would 5 actions: 2 for left and right steering, one for doing nothing, and two for gas and braking.\n",
    "\n",
    "    Environment: \n",
    "\n",
    "        The car has two surfaces to drive over; grass and track. The track can curve and straighten and the grass would cover the rest of the 96x96 screen. The car would also have to obey the physic engine\n",
    "\n",
    "    Reward:\n",
    "\n",
    "        The agent would get a reward for every lap that it accomplishes while touching track tiles and is subtracted by the amount of frames it took to do it. \n",
    "\n",
    "Blackjack (Box2D) - https://gymnasium.farama.org/environments/toy_text/blackjack/\n",
    "\n",
    "    State:\n",
    "\n",
    "        It would be the player's current sum of their hand which could be two depending on whether or not they have an ace. The state would also include the value of the dealer's card.\n",
    "\n",
    "    Action:\n",
    "\n",
    "        There are 2 actions; stand and hit. \n",
    "\n",
    "    Environment: \n",
    "\n",
    "        The player loses whenever the sum of their cards exceed 21 or when the dealer has a higher sum without going over 21. The dealer would have one card face up and one card face down that they only reveal once the player has either stood or busted. Once they reveal then the dealer would hit until they have at least 17 and if they bust then the player wins. \n",
    "\n",
    "    Reward:\n",
    "\n",
    "        The agent would get a reward every time they win and no reward each time they don't win"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "en",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
